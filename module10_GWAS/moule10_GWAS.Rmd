---
title: "module10_GWAS"
author: "Ruijuan Li"
date: "July 20, 2016"
output: html_document
---

# 1) session 1 
```{r}
setwd("/Users/ruijuanli/Desktop/2016_summer/summer_institute/module10_GWAS/")
LHON=read.table("http://faculty.washington.edu/tathornt/sisg/LHON.txt",header=TRUE)
### View the first few lines of the LHON data

head(LHON)

### For computers using Rstudio, could also use the View command #

#View(LHON)


### Take a closer look at the types of variables in the LHON data frame

str(LHON)

# Question 2: Perform logistic regression.  Obtain odds ratios and confidence intervals

# Create a 0 and 1 phenotype variable indicating Case/Control Status to perform a logistic regression analysis


LHON$newpheno=with(LHON,ifelse(PHENO=="CASE",1,0))


## What would be the reference genotype for a logistic regression analysis?  The first factor will be the reference genotype.

levels(LHON$GENO)

# Perform a logistic regression analysis

model1=glm(newpheno~GENO,family=binomial(link="logit"),data=LHON)
?glm

# View the summary results of the logistic regression model, including parameter estimates and standard errors  

summary(model1)

#  Obtain odds ratio and a 95% confidence interval for CT the reference genotype CC  ## 


# summary(model1)
#
#Call:
#glm(formula = newpheno ~ GENO, family = binomial(link = "logit"),data = LHON)
#
#Deviance Residuals: 
#    Min       1Q   Median       3Q      Max  
#-0.9695  -0.8701  -0.8701   1.5197   2.1093  
#
#Coefficients:
#            Estimate Std. Error z value Pr(>|z|)  
#(Intercept)  -0.5108     0.5164  -0.989   0.3226  
#GENOCT       -1.5994     0.6378  -2.508   0.0122 *
#GENOTT       -0.2654     0.5349  -0.496   0.6197  
#---
#Signif. codes:  0 ???***??? 0.001 ???**??? 0.01 ???*??? 0.05 ???.??? 0.1 ??? ??? 1 
#
#(Dispersion parameter for binomial family taken to be 1)
#
#    Null deviance: 383.49  on 327  degrees of freedom
#Residual deviance: 368.48  on 325  degrees of freedom
#AIC: 374.48
#
#Number of Fisher Scoring iterations: 4


## WHAT is the odds ratio for the CT genotype?

exp(-1.5994)

# Obtain a confidence interval for the  odss ratio parameter for the CT genotype

myse=1.96*(.6378)

CI=c(-1.5994-myse,-1.5994+myse)

exp(CI)

# Similarly can obtain a confidence interval for the odds ratio for the TT genotype

exp(-.2654)

myse=1.96*(.5349)

CI=c(-.2654-myse,-.2654+myse)
exp(CI)




## Question 3: Redo the association analysis but use TT as the reference genotype  ##

# Hint: Use the relevel function to create a new genotype vector with reference genotype TT

LHON$NEWGENO=with(LHON,relevel(GENO, ref = "TT"))


levels(LHON$NEWGENO)

### Perform logistic regression using 
model2=glm(newpheno~NEWGENO,family=binomial(link="logit"),data=LHON)
summary(model2) # odd ratio depends on your reference. 


exp(-1.3340 )

myse=1.96*(.3995)

CI=c(-1.3340-myse,-1.3340+myse)
exp(CI)


#  could also do the following to get 95% confidence interval of parameters from your model

MYCI=confint.default(model2)
exp(MYCI)


### Why are the odds ratios different for CT now?  Explain? 
# 1) because you chang ref from CC to TT, odds ratio depends on reference. 
### Plot the data for a better understanding

plot(factor(PHENO)~factor(GENO), data=LHON)
# by looking at the plot, you can find that there is fewer data in CC, sample size 
# is small, which can introduce higher errors. 

#How about an additive logistic regression model?  

LHON$genoadd <- with(LHON, 0 + 1*(GENO=="CT") + 2*(GENO=="TT"))
 
model3 <- glm(newpheno~genoadd,family=binomial(link="logit"),data=LHON)
summary(model3) 


```

# session 2  
```{r}
# session 2 commands

bpdata=read.csv("http://faculty.washington.edu/tathornt/sisg/bpdata.csv",header=TRUE)

# Question 1: linear regression different models

#additive model, only the number of alleles matters 
head(bpdata)
bpdata$n.minor <- (bpdata$snp3=="TC") + (bpdata$snp3=="CT")+ 2*(bpdata$snp3=="TT")
# true as 1 and false as 0 in R 
lm1 = lm(sbp~n.minor, data=bpdata)

# obtain a confidence interval for the linear regression paramaters
confint.default(lm1)
summary(lm1)

#dominant effect of T
bpdata$domvar <- (bpdata$snp3=="TC") | (bpdata$snp3=="TT") # one copy of T allele has 
# same effect of 2 T alleles. 

with(bpdata, table(domvar, snp3)) # table function in R 

lm2 <- lm(sbp~domvar, data=bpdata) 
confint.default(lm2)
summary(lm2)

#Do linear regression analysis assuming a recessive model and assuming a two-degree of freedom model (genotypic model)

# Question 2: Plots
plot(sbp~jitter(n.minor), data=bpdata,col="darkgreen")
abline(lm1, col="red", lwd=2)

boxplot(sbp~n.minor,data=bpdata, col=c("purple","red","darkblue"))
abline(lm1, lwd=2)

## question 3 ##

lm1adj = lm(sbp~n.minor+sex+bmi, data=bpdata) # by including sex, increase the precision of linear regression, get better estimate 
#Get confidence intervals and summary data #

## question 4##
## Perform a linear regression for additive model using all SNPs

# 2 degree of freedom model 
lm3 <- lm(sbp ~ snp3, data = bpdata) # sample size and standard error problem... 
summary(lm3)

# ressesive model 
bpdata$recessive <- (bpdata$snp3=="TT")
lmrec <- lm(sbp ~ recessive, data = bpdata)
summary(lmrec)
```

# session 3
```{r}
setwd("/Users/ruijuanli/Desktop/2016_summer/summer_institute/module10_GWAS/data/SISG_Files/Data_Files/")

## A few R commands for summary statistics for phenotype ##
##  count number of individuals in the study
FAM<-read.table(file="Transferrin.fam",sep=" ", header=FALSE,na="NA")
head(FAM)
dim(FAM)

## Map Information on number of SNPS count number of genotyped SNPs
map<-read.table(file="Transferrin.bim",sep="\t", header=FALSE,na="NA") # SNP data 
head(map) 
dim(map)

## Obtain some basic Info on transferrin phenotype  
TPHENO<-read.table(file="Tr.pheno", header=FALSE)

head(TPHENO) 
dim(TPHENO)

# Give names to PHENO variables  

names(TPHENO)=c("FAMID", "ID", "Transferrin")
head(TPHENO)

###Summary information and histrogram of transferrin pheno

summary(TPHENO$Transferrin)
table(is.na(TPHENO$Transferrin))

hist(TPHENO$Transferrin, xlab="Transferrin",main="Histogram of Transferrin")

## GET INFO on height phenotype  (standardized and adjusted for relevant covariates)
 
HPHENO = read.table(file="Ht.pheno.txt", header=FALSE)

head(HPHENO)
dim(HPHENO)

names(HPHENO)=c("FAMID", "ID", "Height")
head(HPHENO)

###Summary information and histrogram of height
summary(HPHENO$Height)
table(is.na(HPHENO$Height))

hist(HPHENO$Height, xlab="Height",main="Histogram of Height")

##### After you run the PLINK association analysis, use the commands below #######

# If the R package GWASTools has not yet been installed on your computer, install the R package using the commands below 

# source("http://bioconductor.org/biocLite.R")
# biocLite("GWASTools")

### Load the GWASTools package to your R session so that we can use the plotting functions for this package ###

library(GWASTools)

### Read in the Association Results from PLINK for Height

Height.Assoc=read.table("/Users/ruijuanli/Desktop/2016_summer/summer_institute/module10_GWAS/plink-1.07-mac-intel/GWAS_H_add.qassoc",header=TRUE)	

head(Height.Assoc)

#### Obtain a Manhattan plot using GWAS tools ####

manhattanPlot(p=Height.Assoc$P,chromosome=Height.Assoc$CHR, main="Association Results for Height")

### can create a Manhattan plot for just the autosomes ###

AHeight.Assoc=subset(Height.Assoc,CHR<=22)

manhattanPlot(p=AHeight.Assoc$P,chromosome=AHeight.Assoc$CHR, main="Association Results for Autosomes for Height")

#### Obtain a Q-Q plot using GWAS tools ####

qqPlot(pval=Height.Assoc$P)

### Can also us the qqman package to obtain a manhattan plot ###

# install.packages("qqman")
library("qqman")
vignette("qqman")

manhattan(Height.Assoc)
qq(Height.Assoc$P)

### IDENTIFY TOP 10 SNPS  for Height  ###
head(Height.Assoc)
dim(Height.Assoc)		## number of SNPS analyzed

TOP <- Height.Assoc[order(Height.Assoc$P),] 
head(TOP,10)

### Similarly obtain plots and top SNPs for Transferrin ### 
```

# session 4 
```{r}
### R Scripts : Reading data into R and getting rid of missing phenotypes

snps = read.table("/Users/ruijuanli/Desktop/2016_summer/summer_institute/module10_GWAS/plink-1.07-mac-intel/TFsnps.raw", header = T) 
phenotypes = read.table("/Users/ruijuanli/Desktop/2016_summer/summer_institute/module10_GWAS/data/SISG_Files/Data_Files/Tr.pheno", header = F)

missing.phenotypes = which(is.na(phenotypes[,3]))
Z = as.matrix(snps[-missing.phenotypes,7:30])
y = as.vector(phenotypes[-missing.phenotypes,3])


##################################################################################################################
##################################################################################################################


### R Scripts: find out home much missing data we have in the SNPs, and then omit the samples with missing SNPs

sum(is.na(Z)) ## this gives us the number of SNPs that are missing

w.missing = which(apply(is.na(Z),1,sum) > 0) ## This figures out which subjects have ANY missing values

Z = Z[-w.missing,]  ## now we get rid of the subjects with any missing SNPs
y = y[-w.missing]   ## BE CAREFUL: because we are over-writing the variable, don't run this line more than once by accident.

sum(is.na(Z))
sum(is.na(y)) ## Now we see that there is no missingness in either the Z or the y.


##################################################################################################################
##################################################################################################################
### R Scripts: minP tests


pvals = rep(NA, ncol(Z))
for (j in 1:ncol(Z)) {
  pvals[j] = summary(lm(y~Z[,j]))$coef[2,4]  ### this gets the p-value for the association between the j-th snp and y
}


min.pvalue = min(pvals) ## This actually runs the previous function

min.pvalue  ## This is the unadjusted minimum p-value

bonf.pvalue = min(min.pvalue*ncol(Z), 1) ## this is the bonferroni adjusted minimum p-value

### since unadjusted is way too liberal and bonferroni can be way to conservative, let's try getting some sense
### of the "effective number of tests".  We'll try the PCA approach, though many others are possible

getEffectiveSampSize.PCA = function(Z, proportionVariability = 0.99) {
## this function will estimate the number of PCs necessary to explain a certain proportion of the variability
## Z is the matrix of genotypes and proportionVariability is the proportion of variability we want to explain.

  pca = prcomp(Z) ### Does principal component analysis
  evs = pca$sdev**2/sum(pca$sdev**2) ## represents the proportion of variability that is explained by each component
  return(min(which(cumsum(evs)>= proportionVariability)))
}

eff90pct = getEffectiveSampSize.PCA(Z, 0.90)
eff95pct = getEffectiveSampSize.PCA(Z, 0.95)
eff99pct = getEffectiveSampSize.PCA(Z, 0.99)
eff99.9pct = getEffectiveSampSize.PCA(Z, 0.999)

p90pct = min(eff90pct*min.pvalue, 1)
p95pct = min(eff95pct*min.pvalue, 1)
p99pct = min(eff99pct*min.pvalue, 1)
p99.9pct = min(eff99.9pct*min.pvalue, 1)

cbind(c(p90pct, p95pct,p99pct, p99.9pct), c(90,95,99,99.9))

### Note that there is tremendous heterogeneity in the p-values.  Although they are all significant, it's not clear which is the best.

##################################################################################################################
##################################################################################################################
### R Scripts: averaging/collapsing tests

### let's first collapse the SNPs into a simple average or sum of the variants within the region

C = apply(Z, 1, sum) ## This sums each row of Z
hist(C)

summary(lm(y~C))  ## This gives us the association between the averaged/summed SNP value and the outcome y

### Now let's take the top principal component

pca = prcomp(Z)       ### find eigen decomposition
pc1 = Z%*%pca$rot[,1] ### computes the top principal component

summary(lm(y~pc1)) ## Gives us the association


##################################################################################################################
##################################################################################################################
###---***---***---###---***---***---###---***---***---###---***---***---###---***---***---###---***---***---###
###---***---***---###---***---***---###---***---***---###---***---***---###---***---***---###---***---***---###
##################################################################################################################
##################################################################################################################


##################################################################################################################
##################################################################################################################
### R Scripts: Now let's repeat everything with just some random segment of the genome

snps = read.table("/Users/ruijuanli/Desktop/2016_summer/summer_institute/module10_GWAS/plink-1.07-mac-intel/randomSegment.raw", header = T) 
phenotypes = read.table("/Users/ruijuanli/Desktop/2016_summer/summer_institute/module10_GWAS/data/SISG_Files/Data_Files/Tr.pheno", header = F)

missing.phenotypes = which(is.na(phenotypes[,3]))
Z = as.matrix(snps[-missing.phenotypes,7:247])
y = as.vector(phenotypes[-missing.phenotypes,3])



##################################################################################################################
##################################################################################################################
### R Scripts: find out home much missing data we have in the SNPs, and then omit the samples with missing SNPs

sum(is.na(Z)) ## this gives us the number of SNPs that are missing

w.missing = which(apply(is.na(Z),1,sum) > 0) ## This figures out which subjects have ANY missing values

Z = Z[-w.missing,]  ## now we get rid of the subjects with any missing SNPs
y = y[-w.missing]   ## BE CAREFUL: because we are over-writing the variable, don't run this line more than once by accident.

sum(is.na(Z))
sum(is.na(y)) ## Now we see that there is no missingness in either the Z or the y.

##################################################################################################################
##################################################################################################################
### R Scripts: minP tests


pvals = rep(NA, ncol(Z))
for (j in 1:ncol(Z)) {
  pvals[j] = summary(lm(y~Z[,j]))$coef[2,4]  ### this gets the p-value for the association between the j-th snp and y
}


min.pvalue = min(pvals) ## This actually runs the previous function

min.pvalue  ## This is the unadjusted minimum p-value

bonf.pvalue = min(min.pvalue*ncol(Z), 1) ## this is the bonferroni adjusted minimum p-value

### since unadjusted is way too liberal and bonferroni can be way to conservative, let's try getting some sense
### of the "effective number of tests".  We'll try the PCA approach, though many others are possible

getEffectiveSampSize.PCA = function(Z, proportionVariability = 0.99) {
## this function will estimate the number of PCs necessary to explain a certain proportion of the variability
## Z is the matrix of genotypes and proportionVariability is the proportion of variability we want to explain.

  pca = prcomp(Z) ### Does principal component analysis
  evs = pca$sdev**2/sum(pca$sdev**2) ## represents the proportion of variability that is explained by each component
  return(min(which(cumsum(evs)>= proportionVariability)))
}

eff90pct = getEffectiveSampSize.PCA(Z, 0.90)
eff95pct = getEffectiveSampSize.PCA(Z, 0.95)
eff99pct = getEffectiveSampSize.PCA(Z, 0.99)
eff99.9pct = getEffectiveSampSize.PCA(Z, 0.999)

p90pct = min(eff90pct*min.pvalue, 1)
p95pct = min(eff95pct*min.pvalue, 1)
p99pct = min(eff99pct*min.pvalue, 1)
p99.9pct = min(eff99.9pct*min.pvalue, 1)

cbind(c(p90pct, p95pct,p99pct, p99.9pct), c(90,95,99,99.9))

### Note that there is tremendous heterogeneity in the p-values.  Although they are all significant, it's not clear which is the best.

##################################################################################################################
##################################################################################################################
### R Scripts: averaging/collapsing tests

### let's first collapse the SNPs into a simple average or sum of the variants within the region

C = apply(Z, 1, sum) ## This sums each row of Z
hist(C)

summary(lm(y~C))  ## This gives us the association between the averaged/summed SNP value and the outcome y

### Now let's take the top principal component

pca = prcomp(Z)       ### find eigen decomposition
pc1 = Z%*%pca$rot[,1] ### computes the top principal component

summary(lm(y~pc1)) ## Gives us the association
```






